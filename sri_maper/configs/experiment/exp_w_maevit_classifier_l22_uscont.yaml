# @package _global_

defaults:
  - override /data: l22_mvt_tiff.yaml
  - override /model: maevit_classifier.yaml
  - override /trainer: ddp
  - override /callbacks: default.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["w", "cUS", "mae-vit-classifier-cls"]
task_name: "cmta3-classifier-maevit"

seed: 1234
extract_attributions: true

data:
  batch_size: 32
  tif_dir: ${paths.data_dir}/H3/Tungsten-Skarn/
  # left, bottom, right, top bounds in raster's metric
  # below example for eastern continguous US
  predict_bounds: [-2341378, -1760422, 2233339, 1421990] # conterminous US
  # splitting
  samples_per_bin: 2
  # balancing
  multiplier: 20
  downsample: true
  oversample: true
  likely_neg_range: [0.2,0.4]
  log_path: "${paths.output_dir}"

# ckpt_path: logs/cmta3-classifier-maevit/runs/2024-02-03_01-14-57/checkpoints/epoch_006_000000.ckpt

model:
  optimizer:
    _target_: torch.optim.AdamW
    lr: 1e-4
    weight_decay: 1e-3
  net:
    _target_: sri_maper.src.models.mae_vit_classifier.CLSClassifer
    backbone_ckpt: "logs/cmta3-pretrain-maevit/runs/2024-03-14_20-01-15/checkpoints/ssim_0.878.ckpt"
    backbone_net:
      _target_: sri_maper.src.models.mae_vit.MAE_ViT
      image_size: 33
      patch_size: 3
      input_dim: 77
      enc_dim: 256
      encoder_layer: 6
      encoder_head: 8
      dec_dim: 128
      output_dim: 77
      decoder_layer: 2
      decoder_head: 4
      mask_ratio: 0.75
    freeze_backbone: false
    dropout_rate: 0.5
  compile: false
  gain: 1.0
  mc_samples: 100
  smoothing: 0.2
  extract_attributions: ${extract_attributions}
  
trainer:
  min_epochs: 1
  max_epochs: 100
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  limit_test_batches: 1.0
  val_check_interval: 0.5
  # gradient_clip_val: 1.0

logger:
  wandb:
    name: "exp_w_maevit_classifier_l22_uscont"

callbacks:
  model_checkpoint:
    monitor: val/auprc
  early_stopping:
    monitor: val/auprc_best